{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adaptive Testing \n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-fPbd82pXUd0snI5DTCSYT3BlbkFJNT1ZqfvGxFa1K1Pz3bJG\"\n",
    "def generate_questions(prompt, num_questions=10):\n",
    "    try:\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"OPENAI_API_KEY environment variable not set.\")\n",
    "\n",
    "        client = OpenAI(api_key=api_key)\n",
    "        generated_texts = set()\n",
    "        questions = []\n",
    "        i = 0\n",
    "\n",
    "        while i < num_questions:\n",
    "            variation_prompt = f\"{prompt} - Question {i+1}\"\n",
    "\n",
    "            response = client.completions.create(\n",
    "                model=\"gpt-3.5-turbo-instruct\",\n",
    "                prompt=f\"Generate a multiple-choice question about {variation_prompt} strictly in the format:\\n\\n\" \\\n",
    "                       f\"Question:\\n\" \\\n",
    "                       f\"Option A:\\n\" \\\n",
    "                       f\"Option B:\\n\" \\\n",
    "                       f\"Option C:\\n\" \\\n",
    "                       f\"Option D:\\n\" \\\n",
    "                       f\"Correct Option: Option A or Option B or Option C or Option D\",\n",
    "                max_tokens=150,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.9\n",
    "            )\n",
    "\n",
    "            question_text = response.choices[0].text.strip()\n",
    "\n",
    "            if question_text in generated_texts:\n",
    "                continue\n",
    "\n",
    "            generated_texts.add(question_text)\n",
    "\n",
    "            lines = question_text.splitlines()\n",
    "            lines = [line.strip() for line in lines if line.strip()]\n",
    "\n",
    "            if not (lines[0].startswith('Question') and lines[1].startswith('Option A') and\n",
    "                    lines[2].startswith('Option B') and lines[3].startswith('Option C') and\n",
    "                    lines[4].startswith('Option D') and lines[5].startswith('Correct')):\n",
    "                continue\n",
    "\n",
    "            question_data = {\n",
    "                \"Question\": lines[0].replace(\"Question:\", \"\").strip(),\n",
    "                \"Category\": prompt,\n",
    "                \"Option A\": lines[1].replace(\"Option A:\", \"\").strip(),\n",
    "                \"Option B\": lines[2].replace(\"Option B:\", \"\").strip(),\n",
    "                \"Option C\": lines[3].replace(\"Option C:\", \"\").strip(),\n",
    "                \"Option D\": lines[4].replace(\"Option D:\", \"\").strip(),\n",
    "                \"Correct Option\": lines[5].replace(\"Correct Option:\", \"\").strip()[0:8],\n",
    "                'difficulty_index': np.random.uniform(0.1, 1.0),\n",
    "                'discriminatory_index': np.random.uniform(0.1, 1.0),\n",
    "            }\n",
    "\n",
    "            questions.append(question_data)\n",
    "            i += 1\n",
    "\n",
    "        questions_df = pd.DataFrame(questions)\n",
    "        return questions_df\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "                                             Question  Category  \\\n",
      "0   What is the process by which green plants conv...   Biology   \n",
      "1   What process is responsible for converting lig...   Biology   \n",
      "2   What is the process by which plants convert su...   Biology   \n",
      "3   Which process involves the conversion of light...   Biology   \n",
      "4   What is the function of the mitochondria in a ...   Biology   \n",
      "..                                                ...       ...   \n",
      "95  What is the most common form of cancer in wome...  Oncology   \n",
      "96  Which of the following is a common symptom of ...  Oncology   \n",
      "97            What is the most common form of cancer?  Oncology   \n",
      "98  Which of the following is NOT a common treatme...  Oncology   \n",
      "99  What is the most common type of cancer in men ...  Oncology   \n",
      "\n",
      "                                Option A                      Option B  \\\n",
      "0                            Respiration                Photosynthesis   \n",
      "1                         Photosynthesis          Cellular respiration   \n",
      "2                         Photosynthesis          Cellular respiration   \n",
      "3                         Photosynthesis                   Respiration   \n",
      "4   To produce energy in the form of ATP  To store genetic information   \n",
      "..                                   ...                           ...   \n",
      "95                         Breast cancer                   Lung cancer   \n",
      "96                   Shortness of breath                    Runny nose   \n",
      "97                           Lung cancer                 Breast cancer   \n",
      "98                               Surgery             Radiation therapy   \n",
      "99                       Prostate cancer                   Lung cancer   \n",
      "\n",
      "                        Option C                                Option D  \\\n",
      "0                   Fermentation                                 Mitosis   \n",
      "1                        Mitosis                            Fermentation   \n",
      "2                   Fermentation                                 Mitosis   \n",
      "3                   Fermentation                           Transpiration   \n",
      "4   To break down waste products  To transport materials within the cell   \n",
      "..                           ...                                     ...   \n",
      "95             Colorectal cancer                             Skin cancer   \n",
      "96                   Muscle pain                          Irritated skin   \n",
      "97                  Colon cancer                             Skin cancer   \n",
      "98                  Chemotherapy                         Hormone therapy   \n",
      "99             Colorectal cancer                             Skin cancer   \n",
      "\n",
      "   Correct Option  difficulty_index  discriminatory_index  \n",
      "0        Option B          0.701640              0.738263  \n",
      "1        Option A          0.534345              0.434423  \n",
      "2        Option A          0.824227              0.599341  \n",
      "3        Option A          0.267272              0.861935  \n",
      "4        Option A          0.120233              0.923362  \n",
      "..            ...               ...                   ...  \n",
      "95       Option A          0.567702              0.995693  \n",
      "96       Option A          0.430087              0.345541  \n",
      "97       Option A          0.843428              0.495597  \n",
      "98       Option D          0.300283              0.444619  \n",
      "99       Option A          0.620378              0.762632  \n",
      "\n",
      "[100 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# Prompts to generate questions\n",
    "prompts = [\"Biology\", \"Psychology\", \"Gynaecology\", \"Orthopedic\",\"Oncology\"]\n",
    "questions_df_list = [generate_questions(prompt, num_questions=20) for prompt in prompts]\n",
    "all_questions_df = pd.concat(questions_df_list, ignore_index=True)\n",
    "\n",
    "if all_questions_df is not None:\n",
    "    print(\"Generated Questions:\")\n",
    "    print(all_questions_df)\n",
    "else:\n",
    "    print(\"Failed to generate questions.\")\n",
    "\n",
    "all_questions_df=all_questions_df.reset_index().rename(columns={'index':'question_id'})\n",
    "all_questions_df.to_csv('Questions.csv',header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions_df=pd.read_csv('Questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_list = all_questions_df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed! You answered 2 out of 5 questions correctly.\n",
      "Your final score is: 1.09\n",
      "Category 'Oncology' represented 20.00% (1 questions)\n",
      "Category 'Biology' represented 20.00% (1 questions)\n",
      "Category 'Orthopedic' represented 20.00% (1 questions)\n",
      "Category 'Gynaecology' represented 20.00% (1 questions)\n",
      "Category 'Psychology' represented 20.00% (1 questions)\n"
     ]
    }
   ],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from functools import partial\n",
    "\n",
    "class AdaptiveTest:\n",
    "    def __init__(self, questions, total_questions=5):\n",
    "        self.questions = questions\n",
    "        self.total_questions = total_questions\n",
    "        self.asked_questions = []\n",
    "        self.correct_answers = 0\n",
    "        self.correct_difficulty_indices = []\n",
    "        self.current_difficulty = 0.5  # Starting difficulty index\n",
    "        self.categories = list(set(q['Category'] for q in questions))\n",
    "        self.past_categories = []  # List to track past question categories\n",
    "        self.category_counts = {cat: 0 for cat in self.categories}  # Track category counts\n",
    "\n",
    "    def adjust_difficulty(self, previous_difficulty, correct):\n",
    "        if correct:\n",
    "            return min(1.0, previous_difficulty + 0.1)\n",
    "        else:\n",
    "            return max(0.1, previous_difficulty - 0.1)\n",
    "\n",
    "    def setup_milp_model(self, available_questions):\n",
    "        model = gp.Model(\"Question_Selection\")\n",
    "\n",
    "        # Variables\n",
    "        x = model.addVars(len(available_questions), vtype=GRB.BINARY, name=\"x\")\n",
    "\n",
    "        # Objective: Maximize the discriminatory index\n",
    "        obj = gp.quicksum(available_questions[i]['discriminatory_index'] * x[i] for i in range(len(available_questions)))\n",
    "        \n",
    "        # Dynamic penalties for category representation\n",
    "        total_asked = len(self.asked_questions) + 1  # Include the next question to be asked\n",
    "        min_proportion = 0.15 * total_asked\n",
    "        max_proportion = 0.35 * total_asked\n",
    "        \n",
    "        for cat in self.categories:\n",
    "            category_questions = [i for i in range(len(available_questions)) if available_questions[i]['Category'] == cat]\n",
    "            min_needed = max(0, min_proportion - self.category_counts[cat])\n",
    "            max_needed = max_proportion - self.category_counts[cat]\n",
    "            deviation_below = model.addVar(vtype=GRB.CONTINUOUS, name=f\"deviation_below_{cat}\")\n",
    "            deviation_above = model.addVar(vtype=GRB.CONTINUOUS, name=f\"deviation_above_{cat}\")\n",
    "\n",
    "            model.addConstr(deviation_below >= min_needed - gp.quicksum(x[i] for i in category_questions))\n",
    "            model.addConstr(deviation_above >= gp.quicksum(x[i] for i in category_questions) - max_needed)\n",
    "            \n",
    "            obj -= 100 * deviation_below  # Penalize deviation below\n",
    "            obj -= 100 * deviation_above  # Penalize deviation above\n",
    "\n",
    "        model.setObjective(obj, GRB.MAXIMIZE)\n",
    "\n",
    "        # Constraints: Ensure only one question is selected\n",
    "        model.addConstr(gp.quicksum(x[i] for i in range(len(available_questions))) == 1)\n",
    "\n",
    "        return model, x\n",
    "\n",
    "    def get_next_question(self):\n",
    "        available_questions = [q for q in self.questions if q['question_id'] not in self.asked_questions and \n",
    "                               abs(q['difficulty_index'] - self.current_difficulty) <= 0.05]\n",
    "\n",
    "        if not available_questions:\n",
    "            print(\"No more questions available with the current difficulty settings.\")\n",
    "            return None\n",
    "\n",
    "        # Select a question ensuring balanced category representation\n",
    "        model, x = self.setup_milp_model(available_questions)\n",
    "\n",
    "        # Solve the Gurobi model\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            selected_question_index = next(i for i in range(len(available_questions)) if x[i].X > 0.5)\n",
    "            selected_question = available_questions[selected_question_index]\n",
    "            self.asked_questions.append(selected_question['question_id'])\n",
    "            self.past_categories.append(selected_question['Category'])\n",
    "            self.category_counts[selected_question['Category']] += 1  # Update the count for the selected category\n",
    "            return selected_question\n",
    "        else:\n",
    "            print(\"No feasible solution found by the MILP model. Relaxing constraints.\")\n",
    "            self.current_difficulty = self.adjust_difficulty(self.current_difficulty, False)\n",
    "            return self.get_next_question()\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        clear_output(wait=True)  # Clear output before printing question\n",
    "        print(f\"Question: {question['Question']}, Category: {question['Category']}\")  # Print the question\n",
    "        option_buttons = []\n",
    "        for idx, option in enumerate([question['Option A'], question['Option B'], question['Option C'], question['Option D']]):\n",
    "            button = widgets.Button(description=f\"{chr(65+idx)}: {option}\")\n",
    "            button.on_click(partial(self.check_answer, question, chr(65+idx)))\n",
    "            option_buttons.append(button)\n",
    "            button.layout.width = 'auto'\n",
    "            display(button)  # Display the options\n",
    "\n",
    "    def check_answer(self, question, answer, b):\n",
    "        correct_option_letter = question['Correct Option'][-1]  # Get the last character (A, B, C, or D)\n",
    "        correct = (answer == correct_option_letter)\n",
    "        if correct:\n",
    "            self.correct_answers += 1\n",
    "            self.correct_difficulty_indices.append(question['difficulty_index'])\n",
    "\n",
    "        self.current_difficulty = self.adjust_difficulty(self.current_difficulty, correct)\n",
    "\n",
    "        if len(self.asked_questions) < self.total_questions:\n",
    "            next_question = self.get_next_question()\n",
    "            if next_question:\n",
    "                self.ask_question(next_question)\n",
    "            else:\n",
    "                self.finish_test()\n",
    "        else:\n",
    "            self.finish_test()\n",
    "\n",
    "    def finish_test(self):\n",
    "        clear_output(wait=True)\n",
    "        final_score = sum(self.correct_difficulty_indices)\n",
    "        print(f\"Test completed! You answered {self.correct_answers} out of {self.total_questions} questions correctly.\")\n",
    "        print(f\"Your final score is: {final_score:.2f}\")\n",
    "\n",
    "        # Print final category representation\n",
    "        category_counts = {cat: 0 for cat in self.categories}\n",
    "        for q_id in self.asked_questions:\n",
    "            category_counts[self.questions[q_id]['Category']] += 1\n",
    "\n",
    "        total_asked = len(self.asked_questions)\n",
    "        for cat in self.categories:\n",
    "            category_percent = (category_counts[cat] / total_asked) * 100\n",
    "            print(f\"Category '{cat}' represented {category_percent:.2f}% ({category_counts[cat]} questions)\")\n",
    "\n",
    "    def start_test(self):\n",
    "        self.correct_answers = 0\n",
    "        self.asked_questions = []\n",
    "        self.correct_difficulty_indices = []\n",
    "        self.current_difficulty = 0.5  # Reset starting difficulty\n",
    "        self.past_categories = []  # Reset past categories\n",
    "        self.category_counts = {cat: 0 for cat in self.categories}  # Reset category counts\n",
    "        first_question = self.get_next_question()\n",
    "        if first_question:\n",
    "            self.ask_question(first_question)\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the AdaptiveTest class with the questions\n",
    "test = AdaptiveTest(questions_list)\n",
    "test.start_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test completed! You answered 3 out of 5 questions correctly.\n",
      "Your estimated ability level is: 0.10\n",
      "Category 'Oncology' represented 20.00% (1 questions)\n",
      "Category 'Biology' represented 20.00% (1 questions)\n",
      "Category 'Orthopedic' represented 20.00% (1 questions)\n",
      "Category 'Gynaecology' represented 20.00% (1 questions)\n",
      "Category 'Psychology' represented 20.00% (1 questions)\n"
     ]
    }
   ],
   "source": [
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "class AdaptiveTest2PL:\n",
    "    def __init__(self, questions, total_questions=5):\n",
    "        self.questions = questions\n",
    "        self.total_questions = total_questions\n",
    "        self.asked_questions = []\n",
    "        self.correct_answers = 0\n",
    "        self.ability_estimate = 0.0  # Initial ability estimate\n",
    "        self.categories = list(set(q['Category'] for q in questions))\n",
    "        self.past_categories = []  # List to track past question categories\n",
    "        self.category_counts = {cat: 0 for cat in self.categories}  # Track category counts\n",
    "\n",
    "    def adjust_ability(self, previous_ability, correct, question):\n",
    "        # Simple Bayesian updating for ability estimation\n",
    "        learning_rate = 0.1\n",
    "        return previous_ability + learning_rate * (correct - self.probability_correct(previous_ability, question))\n",
    "\n",
    "    def probability_correct(self, ability, question):\n",
    "        # 2PL IRT model: P(correct) = 1 / (1 + exp(-a(theta - b)))\n",
    "        # In this case, we'll use 'discriminatory_index' as 'a' and 'difficulty_index' as 'b'\n",
    "        a = question['discriminatory_index']\n",
    "        b = question['difficulty_index']\n",
    "        return 1 / (1 + np.exp(-a * (ability - b)))\n",
    "\n",
    "    def setup_milp_model(self, available_questions):\n",
    "        model = gp.Model(\"Question_Selection\")\n",
    "\n",
    "        # Variables\n",
    "        x = model.addVars(len(available_questions), vtype=GRB.BINARY, name=\"x\")\n",
    "\n",
    "        # Objective: Maximize the expected information gain\n",
    "        obj = gp.quicksum(self.expected_information_gain(available_questions[i], self.ability_estimate) * x[i] for i in range(len(available_questions)))\n",
    "        \n",
    "        # Dynamic penalties for category representation\n",
    "        total_asked = len(self.asked_questions) + 1  # Include the next question to be asked\n",
    "        min_proportion = 0.15 * total_asked\n",
    "        max_proportion = 0.35 * total_asked\n",
    "        \n",
    "        for cat in self.categories:\n",
    "            category_questions = [i for i in range(len(available_questions)) if available_questions[i]['Category'] == cat]\n",
    "            min_needed = max(0, min_proportion - self.category_counts[cat])\n",
    "            max_needed = max_proportion - self.category_counts[cat]\n",
    "            deviation_below = model.addVar(vtype=GRB.CONTINUOUS, name=f\"deviation_below_{cat}\")\n",
    "            deviation_above = model.addVar(vtype=GRB.CONTINUOUS, name=f\"deviation_above_{cat}\")\n",
    "\n",
    "            model.addConstr(deviation_below >= min_needed - gp.quicksum(x[i] for i in category_questions))\n",
    "            model.addConstr(deviation_above >= gp.quicksum(x[i] for i in category_questions) - max_needed)\n",
    "            \n",
    "            obj -= 100 * deviation_below  # Penalize deviation below\n",
    "            obj -= 100 * deviation_above  # Penalize deviation above\n",
    "\n",
    "        model.setObjective(obj, GRB.MAXIMIZE)\n",
    "\n",
    "        # Constraints: Ensure only one question is selected\n",
    "        model.addConstr(gp.quicksum(x[i] for i in range(len(available_questions))) == 1)\n",
    "\n",
    "        return model, x\n",
    "\n",
    "    def expected_information_gain(self, question, ability):\n",
    "        # Information gain for a 2PL IRT model\n",
    "        p = self.probability_correct(ability, question)\n",
    "        a = question['discriminatory_index']  # Use the existing 'discriminatory_index'\n",
    "        return a**2 * p * (1 - p)\n",
    "\n",
    "    def get_next_question(self):\n",
    "        available_questions = [q for q in self.questions if q['question_id'] not in self.asked_questions]\n",
    "\n",
    "        if not available_questions:\n",
    "            print(\"No more questions available.\")\n",
    "            return None\n",
    "\n",
    "        # Select a question based on expected information gain\n",
    "        model, x = self.setup_milp_model(available_questions)\n",
    "\n",
    "        # Solve the Gurobi model\n",
    "        model.optimize()\n",
    "\n",
    "        if model.status == GRB.OPTIMAL:\n",
    "            selected_question_index = next(i for i in range(len(available_questions)) if x[i].X > 0.5)\n",
    "            selected_question = available_questions[selected_question_index]\n",
    "            self.asked_questions.append(selected_question['question_id'])\n",
    "            self.past_categories.append(selected_question['Category'])\n",
    "            self.category_counts[selected_question['Category']] += 1  # Update the count for the selected category\n",
    "            return selected_question\n",
    "        else:\n",
    "            print(\"No feasible solution found by the MILP model. Relaxing constraints.\")\n",
    "            # If no feasible solution, you might want to implement a fallback strategy \n",
    "            return None\n",
    "\n",
    "    def ask_question(self, question):\n",
    "        clear_output(wait=True)  # Clear output before printing question\n",
    "        print(f\"Question: {question['Question']}, Category: {question['Category']}\")  # Print the question\n",
    "        option_buttons = []\n",
    "        for idx, option in enumerate([question['Option A'], question['Option B'], question['Option C'], question['Option D']]):\n",
    "            button = widgets.Button(description=f\"{chr(65+idx)}: {option}\")\n",
    "            button.on_click(partial(self.check_answer, question, chr(65+idx)))\n",
    "            option_buttons.append(button)\n",
    "            button.layout.width = 'auto'\n",
    "            display(button)  # Display the options\n",
    "\n",
    "    def check_answer(self, question, answer, b):\n",
    "        correct_option_letter = question['Correct Option'][-1]  # Get the last character (A, B, C, or D)\n",
    "        correct = (answer == correct_option_letter)\n",
    "        if correct:\n",
    "            self.correct_answers += 1\n",
    "\n",
    "        self.ability_estimate = self.adjust_ability(self.ability_estimate, correct, question)\n",
    "\n",
    "        if len(self.asked_questions) < self.total_questions:\n",
    "            next_question = self.get_next_question()\n",
    "            if next_question:\n",
    "                self.ask_question(next_question)\n",
    "            else:\n",
    "                self.finish_test()\n",
    "        else:\n",
    "            self.finish_test()\n",
    "\n",
    "    def finish_test(self):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Test completed! You answered {self.correct_answers} out of {self.total_questions} questions correctly.\")\n",
    "        print(f\"Your estimated ability level is: {self.ability_estimate:.2f}\")\n",
    "\n",
    "        # Print final category representation\n",
    "        category_counts = {cat: 0 for cat in self.categories}\n",
    "        for q_id in self.asked_questions:\n",
    "            category_counts[self.questions[q_id]['Category']] += 1\n",
    "\n",
    "        total_asked = len(self.asked_questions)\n",
    "        for cat in self.categories:\n",
    "            category_percent = (category_counts[cat] / total_asked) * 100\n",
    "            print(f\"Category '{cat}' represented {category_percent:.2f}% ({category_counts[cat]} questions)\")\n",
    "\n",
    "    def start_test(self):\n",
    "        self.correct_answers = 0\n",
    "        self.asked_questions = []\n",
    "        self.ability_estimate = 0.0  # Reset starting ability\n",
    "        self.past_categories = []  # Reset past categories\n",
    "        self.category_counts = {cat: 0 for cat in self.categories}  # Reset category counts\n",
    "        first_question = self.get_next_question()\n",
    "        if first_question:\n",
    "            self.ask_question(first_question)\n",
    "\n",
    "# Example usage:\n",
    "# Initialize the AdaptiveTest2PL class with the questions\n",
    "test = AdaptiveTest2PL(questions_list)\n",
    "test.start_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('NN')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5704ccb4069b466c646da1f3ff11884348d327367396c1e7331f1adba0ad30b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
